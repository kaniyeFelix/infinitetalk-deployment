# MCP (Model Context Protocol) ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

MCP æœåŠ¡å™¨æä¾›ç¨‹åºåŒ–è®¿é—® InfiniteTalk çš„æ¥å£ï¼Œé€‚åˆé›†æˆåˆ°å…¶ä»–åº”ç”¨æˆ–è‡ªåŠ¨åŒ–å·¥ä½œæµä¸­ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ MCP æœåŠ¡å™¨

```bash
# æ–¹å¼ä¸€ï¼šç›´æ¥è¿è¡Œ
python3 mcp_server.py

# æ–¹å¼äºŒï¼šé€šè¿‡ Docker
docker exec infinitetalk python3 mcp_server.py
```

### 2. é…ç½® MCP å®¢æˆ·ç«¯

åœ¨ä½ çš„ MCP å®¢æˆ·ç«¯é…ç½®æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```json
{
  "mcpServers": {
    "infinitetalk": {
      "command": "python3",
      "args": ["/path/to/mcp_server.py"],
      "env": {
        "GPU_IDLE_TIMEOUT": "600"
      }
    }
  }
}
```

## ğŸ› ï¸ å¯ç”¨å·¥å…·

### 1. process_image_to_video

å›¾ç‰‡è½¬è§†é¢‘ç”Ÿæˆ

**å‚æ•°ï¼š**
- `image_path` (string, required): è¾“å…¥å›¾ç‰‡è·¯å¾„
- `audio_path` (string, required): è¾“å…¥éŸ³é¢‘è·¯å¾„
- `text_prompt` (string, optional): æ–‡æœ¬æç¤º
- `model_type` (string, optional): æ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ "single_original"

**è¿”å›ï¼š**
```json
{
  "status": "success",
  "output_path": "/path/to/output.mp4",
  "model_used": "single_original",
  "message": "è§†é¢‘ç”Ÿæˆå®Œæˆ"
}
```

**ç¤ºä¾‹ï¼š**
```python
result = await mcp_client.call_tool(
    "process_image_to_video",
    {
        "image_path": "/path/to/image.jpg",
        "audio_path": "/path/to/audio.wav",
        "text_prompt": "A person talking",
        "model_type": "single_original"
    }
)
```

### 2. process_video_to_video

è§†é¢‘è½¬è§†é¢‘ï¼ˆå£å‹åŒæ­¥ï¼‰

**å‚æ•°ï¼š**
- `video_path` (string, required): è¾“å…¥è§†é¢‘è·¯å¾„
- `audio_path` (string, required): è¾“å…¥éŸ³é¢‘è·¯å¾„
- `text_prompt` (string, optional): æ–‡æœ¬æç¤º
- `model_type` (string, optional): æ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ "multi_original"

**è¿”å›ï¼š**
```json
{
  "status": "success",
  "output_path": "/path/to/output.mp4",
  "model_used": "multi_original",
  "message": "è§†é¢‘åŒæ­¥å®Œæˆ"
}
```

### 3. get_gpu_status

è·å– GPU çŠ¶æ€ä¿¡æ¯

**å‚æ•°ï¼š** æ— 

**è¿”å›ï¼š**
```json
{
  "model_location": "GPU",
  "idle_time": 30,
  "idle_timeout": 60,
  "gpu_available": true,
  "gpu_memory_allocated_gb": 8.5,
  "gpu_memory_reserved_gb": 9.0,
  "gpu_name": "NVIDIA GeForce RTX 4090"
}
```

### 4. offload_gpu

æ‰‹åŠ¨å¸è½½ GPU æ˜¾å­˜

**å‚æ•°ï¼š** æ— 

**è¿”å›ï¼š**
```json
{
  "status": "offloaded",
  "message": "æ¨¡å‹å·²å¸è½½åˆ° CPUï¼Œæ˜¾å­˜å·²é‡Šæ”¾"
}
```

### 5. release_gpu

å®Œå…¨é‡Šæ”¾ GPU å’Œ CPU ç¼“å­˜

**å‚æ•°ï¼š** æ— 

**è¿”å›ï¼š**
```json
{
  "status": "released",
  "message": "æ¨¡å‹å·²å®Œå…¨é‡Šæ”¾"
}
```

### 6. update_gpu_timeout

æ›´æ–° GPU ç©ºé—²è¶…æ—¶æ—¶é—´

**å‚æ•°ï¼š**
- `timeout_seconds` (integer, required): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

**è¿”å›ï¼š**
```json
{
  "status": "updated",
  "timeout": 300,
  "message": "ç©ºé—²è¶…æ—¶å·²æ›´æ–°ä¸º 300ç§’"
}
```

### 7. list_available_models

åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹

**å‚æ•°ï¼š** æ— 

**è¿”å›ï¼š**
```json
{
  "status": "success",
  "models": {
    "infinitetalk": [
      "single_original",
      "multi_original",
      "single_int8",
      "multi_int8",
      "single_fp8",
      "multi_fp8"
    ],
    "multitalk": [
      "multitalk_original",
      "multitalk_int8_fusionx",
      "multitalk_fp8_fusionx"
    ]
  },
  "total_count": 9
}
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Python å®¢æˆ·ç«¯

```python
from mcp import ClientSession
import asyncio

async def main():
    async with ClientSession() as session:
        # 1. æ£€æŸ¥ GPU çŠ¶æ€
        status = await session.call_tool("get_gpu_status", {})
        print(f"GPU çŠ¶æ€: {status}")
        
        # 2. ç”Ÿæˆè§†é¢‘
        result = await session.call_tool(
            "process_image_to_video",
            {
                "image_path": "/path/to/image.jpg",
                "audio_path": "/path/to/audio.wav",
                "model_type": "single_original"
            }
        )
        print(f"ç”Ÿæˆç»“æœ: {result}")
        
        # 3. æ‰‹åŠ¨å¸è½½ GPU
        await session.call_tool("offload_gpu", {})

asyncio.run(main())
```

### CLI è°ƒç”¨

```bash
# ä½¿ç”¨ mcp CLI
mcp call process_image_to_video '{
  "image_path": "/path/to/image.jpg",
  "audio_path": "/path/to/audio.wav"
}'

# æŸ¥çœ‹ GPU çŠ¶æ€
mcp call get_gpu_status '{}'

# æ‰‹åŠ¨å¸è½½
mcp call offload_gpu '{}'
```

## ğŸ”„ MCP vs API å¯¹æ¯”

| ç‰¹æ€§ | MCP | REST API |
|------|-----|----------|
| è®¿é—®æ–¹å¼ | ç¨‹åºåŒ–è°ƒç”¨ | HTTP è¯·æ±‚ |
| é€‚ç”¨åœºæ™¯ | è‡ªåŠ¨åŒ–ã€é›†æˆ | Web åº”ç”¨ã€æµ‹è¯• |
| ç±»å‹å®‰å…¨ | âœ… å¼ºç±»å‹ | âš ï¸ éœ€éªŒè¯ |
| æ–‡æ¡£ | è‡ªåŠ¨ç”Ÿæˆ | Swagger |
| æ€§èƒ½ | ğŸš€ æ›´å¿« | æ ‡å‡† |

## ğŸ’¡ æœ€ä½³å®è·µ

1. **GPU ç®¡ç†**
   - å¤„ç†å®Œæˆåç«‹å³è°ƒç”¨ `offload_gpu`
   - é•¿æ—¶é—´ä¸ç”¨æ—¶è°ƒç”¨ `release_gpu`
   - å®šæœŸæ£€æŸ¥ `get_gpu_status`

2. **é”™è¯¯å¤„ç†**
   - å§‹ç»ˆæ£€æŸ¥è¿”å›çš„ `status` å­—æ®µ
   - æ•è·å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—
   - å¤±è´¥æ—¶ç¡®ä¿è°ƒç”¨ `offload_gpu`

3. **æ€§èƒ½ä¼˜åŒ–**
   - æ‰¹é‡å¤„ç†æ—¶å¤ç”¨æ¨¡å‹
   - åˆç†è®¾ç½® `GPU_IDLE_TIMEOUT`
   - ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ï¼šMCP æœåŠ¡å™¨æ— æ³•å¯åŠ¨

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ä¾èµ–
pip3 install fastmcp

# æŸ¥çœ‹æ—¥å¿—
python3 mcp_server.py 2>&1 | tee mcp.log
```

### é—®é¢˜ï¼šGPU æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# ç«‹å³é‡Šæ”¾æ˜¾å­˜
await session.call_tool("release_gpu", {})

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
result = await session.call_tool(
    "process_image_to_video",
    {"model_type": "single_original"}  # ä½¿ç”¨åŸç‰ˆæ¨¡å‹
)
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [GPU ç®¡ç†æ–‡æ¡£](GPU_MANAGEMENT.md)
- [API æ–‡æ¡£](http://localhost:7860/docs)
- [é¡¹ç›® README](README.md)

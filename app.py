import gradio as gr
import os
import gc
import torch
from threading import Timer, Thread
import subprocess
import time

# æ‰€æœ‰å¯ç”¨æ¨¡å‹é…ç½®
MODELS = {
    # InfiniteTalk åŸç‰ˆæ¨¡å‹
    "single_original": {
        "name": "â­ å•äººæ¨¡å¼ï¼ˆåŸç‰ˆï¼‰",
        "path": "/app/models/single/infinitetalk.safetensors",
        "description": "åŸç‰ˆå•äººæ¨¡å¼ï¼Œä½“ç§¯å°ï¼Œé€Ÿåº¦å¿«",
        "size": "9.95GB",
        "type": "InfiniteTalk",
        "quality": "æ ‡å‡†",
        "recommended": True,
        "use_case": "é€‚åˆå•äººè¯´è¯è§†é¢‘ï¼Œæ¨èæ–°æ‰‹ä½¿ç”¨"
    },
    "multi_original": {
        "name": "â­ å¤šäººæ¨¡å¼ï¼ˆåŸç‰ˆï¼‰",
        "path": "/app/models/multi/infinitetalk.safetensors",
        "description": "åŸç‰ˆå¤šäººæ¨¡å¼ï¼Œæ”¯æŒå¤šäººå¯¹è¯åœºæ™¯",
        "size": "9.95GB",
        "type": "InfiniteTalk",
        "quality": "æ ‡å‡†",
        "recommended": True,
        "use_case": "é€‚åˆå¤šäººå¯¹è¯è§†é¢‘ï¼Œæ¨èæ–°æ‰‹ä½¿ç”¨"
    },
    
    # InfiniteTalk INT8 é‡åŒ–æ¨¡å‹
    "single_int8": {
        "name": "å•äººæ¨¡å¼ INT8",
        "path": "/app/models/quant_models/infinitetalk_single_int8.safetensors",
        "description": "INT8é‡åŒ–ï¼Œè´¨é‡æ›´é«˜ä½†ä½“ç§¯æ›´å¤§",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡",
        "recommended": False,
        "use_case": "è¿½æ±‚é«˜è´¨é‡å•äººè§†é¢‘"
    },
    "single_int8_lora": {
        "name": "å•äººæ¨¡å¼ INT8 LoRA",
        "path": "/app/models/quant_models/infinitetalk_single_int8_lora.safetensors",
        "description": "INT8 + LoRAï¼Œæ”¯æŒé£æ ¼æ§åˆ¶",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡+é£æ ¼",
        "recommended": False,
        "use_case": "éœ€è¦é£æ ¼æ§åˆ¶çš„å•äººè§†é¢‘"
    },
    "multi_int8": {
        "name": "å¤šäººæ¨¡å¼ INT8",
        "path": "/app/models/quant_models/infinitetalk_multi_int8.safetensors",
        "description": "INT8é‡åŒ–ï¼Œå¤šäººé«˜è´¨é‡",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡",
        "recommended": False,
        "use_case": "è¿½æ±‚é«˜è´¨é‡å¤šäººè§†é¢‘"
    },
    "multi_int8_lora": {
        "name": "å¤šäººæ¨¡å¼ INT8 LoRA",
        "path": "/app/models/quant_models/infinitetalk_multi_int8_lora.safetensors",
        "description": "INT8 + LoRAï¼Œå¤šäººé£æ ¼æ§åˆ¶",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡+é£æ ¼",
        "recommended": False,
        "use_case": "éœ€è¦é£æ ¼æ§åˆ¶çš„å¤šäººè§†é¢‘"
    },
    
    # InfiniteTalk FP8 é‡åŒ–æ¨¡å‹
    "single_fp8": {
        "name": "å•äººæ¨¡å¼ FP8",
        "path": "/app/models/quant_models/infinitetalk_single_fp8.safetensors",
        "description": "FP8é‡åŒ–ï¼Œå¹³è¡¡è´¨é‡ä¸é€Ÿåº¦",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡",
        "recommended": False,
        "use_case": "å¹³è¡¡è´¨é‡ä¸æ€§èƒ½çš„å•äººè§†é¢‘"
    },
    "multi_fp8": {
        "name": "å¤šäººæ¨¡å¼ FP8",
        "path": "/app/models/quant_models/infinitetalk_multi_fp8.safetensors",
        "description": "FP8é‡åŒ–ï¼Œå¤šäººå¹³è¡¡æ¨¡å¼",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡",
        "recommended": False,
        "use_case": "å¹³è¡¡è´¨é‡ä¸æ€§èƒ½çš„å¤šäººè§†é¢‘"
    },
    "multi_fp8_lora": {
        "name": "å¤šäººæ¨¡å¼ FP8 LoRA",
        "path": "/app/models/quant_models/infinitetalk_multi_fp8_lora.safetensors",
        "description": "FP8 + LoRAï¼Œå¤šäººé£æ ¼å¹³è¡¡",
        "size": "19.5GB",
        "type": "InfiniteTalk",
        "quality": "é«˜è´¨é‡+é£æ ¼",
        "recommended": False,
        "use_case": "éœ€è¦é£æ ¼æ§åˆ¶ä¸”æ³¨é‡æ€§èƒ½çš„å¤šäººè§†é¢‘"
    },
    
    # T5 æ¨¡å‹
    "t5_fp8": {
        "name": "T5 FP8ï¼ˆè¾…åŠ©ï¼‰",
        "path": "/app/models/quant_models/t5_fp8.safetensors",
        "description": "T5æ–‡æœ¬ç¼–ç å™¨ï¼Œè¾…åŠ©æ¨¡å‹",
        "size": "6.73GB",
        "type": "InfiniteTalk",
        "quality": "è¾…åŠ©",
        "recommended": False,
        "use_case": "é…åˆä¸»æ¨¡å‹ä½¿ç”¨çš„æ–‡æœ¬ç¼–ç å™¨"
    },
    
    # MeiGen-MultiTalk æ¨¡å‹
    "multitalk_original": {
        "name": "ğŸ­ MultiTalkï¼ˆåŸç‰ˆï¼‰",
        "path": "/app/models/multitalk/multitalk.safetensors",
        "description": "MultiTalkåŸç‰ˆï¼Œå¤šäººå¯¹è¯ä¸“ç”¨",
        "size": "9.95GB",
        "type": "MultiTalk",
        "quality": "æ ‡å‡†",
        "recommended": False,
        "use_case": "ä¸“æ³¨äºå¤šäººå¯¹è¯åœºæ™¯ï¼Œæ”¯æŒæ›´å¤æ‚çš„äº¤äº’"
    },
    "multitalk_int8": {
        "name": "MultiTalk INT8",
        "path": "/app/models/multitalk/quant_models/dit_model_int8.safetensors",
        "description": "MultiTalk INT8é‡åŒ–",
        "size": "19.1GB",
        "type": "MultiTalk",
        "quality": "é«˜è´¨é‡",
        "recommended": False,
        "use_case": "é«˜è´¨é‡å¤šäººå¯¹è¯è§†é¢‘"
    },
    "multitalk_int8_fusion": {
        "name": "MultiTalk INT8 FusionX",
        "path": "/app/models/multitalk/quant_models/quant_model_int8_FusionX.safetensors",
        "description": "MultiTalk INT8 + FusionXåŠ é€Ÿ",
        "size": "19.1GB",
        "type": "MultiTalk",
        "quality": "é«˜è´¨é‡+å¿«é€Ÿ",
        "recommended": False,
        "use_case": "éœ€è¦å¿«é€Ÿç”Ÿæˆçš„é«˜è´¨é‡å¤šäººè§†é¢‘"
    },
    "multitalk_fp8_fusion": {
        "name": "MultiTalk FP8 FusionX",
        "path": "/app/models/multitalk/quant_models/quant_model_fp8_FusionX.safetensors",
        "description": "MultiTalk FP8 + FusionXåŠ é€Ÿ",
        "size": "19.1GB",
        "type": "MultiTalk",
        "quality": "é«˜è´¨é‡+å¿«é€Ÿ",
        "recommended": False,
        "use_case": "å¹³è¡¡è´¨é‡ä¸é€Ÿåº¦çš„å¤šäººè§†é¢‘"
    },
    "multitalk_t5_int8": {
        "name": "MultiTalk T5 INT8",
        "path": "/app/models/multitalk/quant_models/t5_int8.safetensors",
        "description": "MultiTalk T5ç¼–ç å™¨ INT8",
        "size": "6.73GB",
        "type": "MultiTalk",
        "quality": "è¾…åŠ©",
        "recommended": False,
        "use_case": "é…åˆMultiTalkä½¿ç”¨"
    },
    "multitalk_t5_fp8": {
        "name": "MultiTalk T5 FP8",
        "path": "/app/models/multitalk/quant_models/t5_fp8.safetensors",
        "description": "MultiTalk T5ç¼–ç å™¨ FP8",
        "size": "6.73GB",
        "type": "MultiTalk",
        "quality": "è¾…åŠ©",
        "recommended": False,
        "use_case": "é…åˆMultiTalkä½¿ç”¨"
    }
}

current_model = None
model_pipeline = None
idle_timer = None
IDLE_TIMEOUT = 300
download_process = None

def get_file_size_gb(filepath):
    """è·å–æ–‡ä»¶å¤§å°ï¼ˆGBï¼‰"""
    if os.path.exists(filepath):
        size_bytes = os.path.getsize(filepath)
        return size_bytes / (1000**3)  # ä½¿ç”¨GBè€Œä¸æ˜¯GiB
    return 0

def check_and_download_models():
    """æ£€æŸ¥å¹¶è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹"""
    missing_models = []
    for key, info in MODELS.items():
        if not os.path.exists(info["path"]):
            missing_models.append((key, info))
    
    if missing_models:
        print(f"å‘ç° {len(missing_models)} ä¸ªæ¨¡å‹éœ€è¦ä¸‹è½½")
        # å¯åŠ¨åå°ä¸‹è½½
        Thread(target=download_missing_models, args=(missing_models,), daemon=True).start()

def download_missing_models(models_list):
    """åå°ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹"""
    for key, info in models_list:
        print(f"å¼€å§‹ä¸‹è½½: {info['name']}")
        # è¿™é‡Œæ·»åŠ å®é™…çš„ä¸‹è½½é€»è¾‘
        # ä½¿ç”¨ wget æˆ– huggingface-cli ä¸‹è½½
        time.sleep(1)  # å ä½ç¬¦

def get_download_progress():
    """è·å–ä¸‹è½½è¿›åº¦"""
    progress_info = []
    
    for key, info in MODELS.items():
        path = info["path"]
        expected_size = float(info["size"].replace("GB", ""))
        
        if os.path.exists(path):
            current_size = get_file_size_gb(path)
            if current_size < expected_size * 0.95:  # æœªå®Œæˆä¸‹è½½
                progress = (current_size / expected_size) * 100
                progress_info.append(f"â³ {info['name']}: {current_size:.1f}GB / {expected_size}GB ({progress:.1f}%)")
            else:
                progress_info.append(f"âœ… {info['name']}: å·²å®Œæˆ")
        else:
            progress_info.append(f"â¸ï¸ {info['name']}: æœªå¼€å§‹")
    
    return "\n".join(progress_info) if progress_info else "æ‰€æœ‰æ¨¡å‹å·²å°±ç»ª"

def check_model_exists(model_key):
    return os.path.exists(MODELS[model_key]["path"])

def get_model_status():
    infinitetalk_list = []
    multitalk_list = []
    
    for key, info in MODELS.items():
        exists = check_model_exists(key)
        status = "âœ…" if exists else "â³"
        line = f"{status} {info['name']} ({info['size']})"
        
        if info["type"] == "InfiniteTalk":
            infinitetalk_list.append(line)
        else:
            multitalk_list.append(line)
    
    result = "=== InfiniteTalk æ¨¡å‹ ===\n" + "\n".join(infinitetalk_list)
    result += "\n\n=== MultiTalk æ¨¡å‹ ===\n" + "\n".join(multitalk_list)
    return result

def unload_model():
    global model_pipeline, current_model
    if model_pipeline is not None:
        del model_pipeline
        model_pipeline = None
        current_model = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("æ¨¡å‹å·²ä»æ˜¾å­˜ä¸­é‡Šæ”¾")

def reset_idle_timer():
    global idle_timer
    if idle_timer:
        idle_timer.cancel()
    idle_timer = Timer(IDLE_TIMEOUT, unload_model)
    idle_timer.start()

def load_model(model_key):
    global current_model
    if not model_key:
        return "âš ï¸ è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹"
    
    if not check_model_exists(model_key):
        return f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·ç­‰å¾…ä¸‹è½½å®Œæˆ\nè·¯å¾„: {MODELS[model_key]['path']}"
    
    try:
        if current_model and current_model != model_key:
            unload_model()
        
        current_model = model_key
        reset_idle_timer()
        info = MODELS[model_key]
        return f"âœ… å·²åŠ è½½: {info['name']}\nç±»å‹: {info['type']}\nè´¨é‡: {info['quality']}\nè·¯å¾„: {info['path']}\n\nğŸ’¡ æç¤ºï¼š5åˆ†é’Ÿä¸ä½¿ç”¨å°†è‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜"
    except Exception as e:
        return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"

def generate_video(mode, input_image, input_video, input_audio, input_text):
    if not current_model:
        return None, "âš ï¸ è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹"
    
    reset_idle_timer()
    info = MODELS[current_model]
    
    if mode == "image_to_video":
        if not input_image or not input_audio:
            return None, "âš ï¸ å›¾ç‰‡è½¬è§†é¢‘æ¨¡å¼éœ€è¦ä¸Šä¼ å›¾ç‰‡å’ŒéŸ³é¢‘"
        return None, f"ä½¿ç”¨ {info['name']} ç”Ÿæˆè§†é¢‘\næ¨¡å¼: å›¾ç‰‡è½¬è§†é¢‘\nç±»å‹: {info['type']}\næ–‡æœ¬: {input_text[:50] if input_text else 'æ— '}..."
    else:
        if not input_video or not input_audio:
            return None, "âš ï¸ è§†é¢‘è½¬è§†é¢‘æ¨¡å¼éœ€è¦ä¸Šä¼ è§†é¢‘å’ŒéŸ³é¢‘"
        return None, f"ä½¿ç”¨ {info['name']} ç”Ÿæˆè§†é¢‘\næ¨¡å¼: è§†é¢‘è½¬è§†é¢‘ï¼ˆå£å‹åŒæ­¥ï¼‰\nç±»å‹: {info['type']}\næ–‡æœ¬: {input_text[:50] if input_text else 'æ— '}..."

with gr.Blocks(title="InfiniteTalk & MultiTalk è§†é¢‘ç”Ÿæˆ") as demo:
    gr.Markdown("# ğŸ¬ InfiniteTalk & MultiTalk éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆç³»ç»Ÿ")
    gr.Markdown("æ”¯æŒå›¾ç‰‡è½¬è§†é¢‘å’Œè§†é¢‘å£å‹åŒæ­¥ï¼Œæ— é™é•¿åº¦è§†é¢‘ç”Ÿæˆ | â­ æ ‡è®°ä¸ºæ¨èæ¨¡å‹")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¦ æ¨¡å‹ç®¡ç†")
            
            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
            infinitetalk_choices = [(v["name"], k) for k, v in MODELS.items() if v["type"] == "InfiniteTalk"]
            multitalk_choices = [(v["name"], k) for k, v in MODELS.items() if v["type"] == "MultiTalk"]
            
            model_type = gr.Radio(
                choices=["InfiniteTalk", "MultiTalk"],
                label="æ¨¡å‹ç³»åˆ—",
                value="InfiniteTalk"
            )
            
            model_dropdown = gr.Dropdown(
                choices=infinitetalk_choices,
                label="é€‰æ‹©æ¨¡å‹",
                value=None
            )
            
            model_info = gr.Textbox(label="æ¨¡å‹è¯¦æƒ…", interactive=False, lines=6)
            
            with gr.Row():
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                refresh_btn = gr.Button("ğŸ” åˆ·æ–°çŠ¶æ€")
                unload_btn = gr.Button("ğŸ—‘ï¸ é‡Šæ”¾æ˜¾å­˜")
            
            status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False, lines=5)
            model_status = gr.Textbox(label="æ‰€æœ‰æ¨¡å‹çŠ¶æ€", interactive=False, lines=18, value=get_model_status())
            
            gr.Markdown("### ğŸ“¥ ä¸‹è½½è¿›åº¦")
            download_progress = gr.Textbox(label="å®æ—¶ä¸‹è½½è¿›åº¦", interactive=False, lines=10, value=get_download_progress())
            refresh_download_btn = gr.Button("ğŸ”„ åˆ·æ–°ä¸‹è½½è¿›åº¦")
            
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ¥ è§†é¢‘ç”Ÿæˆ")
            
            mode = gr.Radio(
                choices=[("å›¾ç‰‡è½¬è§†é¢‘", "image_to_video"), ("è§†é¢‘è½¬è§†é¢‘ï¼ˆå£å‹åŒæ­¥ï¼‰", "video_to_video")],
                label="ç”Ÿæˆæ¨¡å¼",
                value="image_to_video"
            )
            
            with gr.Row():
                with gr.Column():
                    input_image = gr.Image(label="è¾“å…¥å›¾ç‰‡ï¼ˆå›¾ç‰‡è½¬è§†é¢‘æ¨¡å¼ï¼‰", type="filepath")
                    input_video = gr.Video(label="è¾“å…¥è§†é¢‘ï¼ˆè§†é¢‘è½¬è§†é¢‘æ¨¡å¼ï¼‰")
                
                with gr.Column():
                    input_audio = gr.Audio(label="è¾“å…¥éŸ³é¢‘ï¼ˆå¿…éœ€ï¼‰", type="filepath")
                    input_text = gr.Textbox(label="æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰", placeholder="è¾“å…¥æè¿°æ–‡æœ¬...", lines=3)
            
            generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
            
            output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
            output_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False, lines=3)
    
    with gr.Accordion("ğŸ“‹ æ¨¡å‹è¯¦ç»†è¯´æ˜", open=False):
        gr.Markdown("""
        ## InfiniteTalk ç³»åˆ—ï¼ˆæ¨èæ–°æ‰‹ï¼‰
        
        ### â­ æ¨èæ¨¡å‹
        - **å•äººæ¨¡å¼ï¼ˆåŸç‰ˆï¼‰**: ä½“ç§¯å°ï¼ˆ9.95GBï¼‰ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆå•äººè¯´è¯è§†é¢‘ï¼Œ**æ¨èæ–°æ‰‹é¦–é€‰**
        - **å¤šäººæ¨¡å¼ï¼ˆåŸç‰ˆï¼‰**: ä½“ç§¯å°ï¼ˆ9.95GBï¼‰ï¼Œæ”¯æŒå¤šäººå¯¹è¯ï¼Œ**æ¨èå¤šäººåœºæ™¯é¦–é€‰**
        
        ### é«˜è´¨é‡æ¨¡å‹ï¼ˆè¿½æ±‚è´¨é‡ï¼‰
        - **INT8 ç³»åˆ—**: ä½“ç§¯å¤§ï¼ˆ19.5GBï¼‰ï¼Œè´¨é‡æ›´é«˜ï¼Œé€‚åˆè¿½æ±‚é«˜è´¨é‡è¾“å‡º
        - **FP8 ç³»åˆ—**: ä½“ç§¯å¤§ï¼ˆ19.5GBï¼‰ï¼Œå¹³è¡¡è´¨é‡ä¸é€Ÿåº¦
        - **LoRA ç³»åˆ—**: æ”¯æŒé£æ ¼æ§åˆ¶ï¼Œé€‚åˆéœ€è¦ç‰¹å®šé£æ ¼çš„åœºæ™¯
        
        ### è¾…åŠ©æ¨¡å‹
        - **T5 FP8**: æ–‡æœ¬ç¼–ç å™¨ï¼Œé…åˆä¸»æ¨¡å‹ä½¿ç”¨
        
        ---
        
        ## MultiTalk ç³»åˆ—ï¼ˆä¸“ä¸šç”¨æˆ·ï¼‰
        
        ### ç‰¹ç‚¹
        - ä¸“æ³¨äºå¤šäººå¯¹è¯åœºæ™¯
        - æ”¯æŒæ›´å¤æ‚çš„äº¤äº’æ§åˆ¶
        - æä¾› FusionX åŠ é€Ÿç‰ˆæœ¬
        
        ### æ¨¡å‹é€‰æ‹©
        - **åŸç‰ˆ**: æ ‡å‡†è´¨é‡ï¼Œé€‚åˆæµ‹è¯•
        - **INT8/FP8 FusionX**: é«˜è´¨é‡ + å¿«é€Ÿç”Ÿæˆ
        - **T5 æ¨¡å‹**: é…åˆä¸»æ¨¡å‹ä½¿ç”¨
        
        ---
        
        ## ä½¿ç”¨å»ºè®®
        
        1. **æ–°æ‰‹**: ä½¿ç”¨ â­ æ ‡è®°çš„åŸç‰ˆæ¨¡å‹
        2. **è¿½æ±‚è´¨é‡**: ä½¿ç”¨ INT8 æˆ– FP8 é‡åŒ–æ¨¡å‹
        3. **éœ€è¦é£æ ¼**: ä½¿ç”¨ LoRA ç³»åˆ—
        4. **å¤šäººå¯¹è¯**: InfiniteTalk å¤šäººæ¨¡å¼æˆ– MultiTalk
        5. **å¿«é€Ÿç”Ÿæˆ**: MultiTalk FusionX ç³»åˆ—
        """)
    
    with gr.Accordion("ğŸ’¡ ä½¿ç”¨è¯´æ˜", open=False):
        gr.Markdown("""
        ### åŠŸèƒ½è¯´æ˜
        - **å›¾ç‰‡è½¬è§†é¢‘**: è¾“å…¥ä¸€å¼ å›¾ç‰‡å’ŒéŸ³é¢‘ï¼Œç”Ÿæˆè¯´è¯çš„è§†é¢‘
        - **è§†é¢‘è½¬è§†é¢‘**: è¾“å…¥è§†é¢‘å’Œæ–°éŸ³é¢‘ï¼Œç”Ÿæˆå£å‹åŒæ­¥çš„æ–°è§†é¢‘
        
        ### æ˜¾å­˜ç®¡ç†
        - æ¨¡å‹åŠ è½½åä¼šå ç”¨æ˜¾å­˜
        - **5åˆ†é’Ÿä¸ä½¿ç”¨ä¼šè‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜**
        - å¯æ‰‹åŠ¨ç‚¹å‡»"é‡Šæ”¾æ˜¾å­˜"æŒ‰é’®ç«‹å³é‡Šæ”¾
        
        ### æ¨¡å‹å¤§å°è¯´æ˜
        - **åŸç‰ˆæ¨¡å‹**: 9.95GBï¼Œé€Ÿåº¦å¿«ï¼Œè´¨é‡æ ‡å‡†
        - **é‡åŒ–æ¨¡å‹**: 19.5GBï¼Œè´¨é‡æ›´é«˜ï¼Œé€Ÿåº¦ç¨æ…¢
        - **T5æ¨¡å‹**: 6.73GBï¼Œè¾…åŠ©æ¨¡å‹
        """)
    
    def update_model_dropdown(model_type):
        if model_type == "InfiniteTalk":
            choices = infinitetalk_choices
        else:
            choices = multitalk_choices
        return gr.Dropdown(choices=choices, value=None)
    
    def update_model_info(model_key):
        if model_key:
            info = MODELS[model_key]
            exists = check_model_exists(model_key)
            status_icon = "âœ…" if exists else "â³"
            rec = "â­ æ¨è" if info["recommended"] else ""
            return f"{status_icon} {rec}\n\nç±»å‹: {info['type']}\nè´¨é‡: {info['quality']}\nå¤§å°: {info['size']}\n\nè¯´æ˜: {info['description']}\n\né€‚ç”¨åœºæ™¯: {info['use_case']}\n\nè·¯å¾„: {info['path']}"
        return ""
    
    model_type.change(fn=update_model_dropdown, inputs=[model_type], outputs=[model_dropdown])
    model_dropdown.change(fn=update_model_info, inputs=[model_dropdown], outputs=[model_info])
    load_btn.click(fn=load_model, inputs=[model_dropdown], outputs=[status])
    refresh_btn.click(fn=get_model_status, inputs=[], outputs=[model_status])
    refresh_download_btn.click(fn=get_download_progress, inputs=[], outputs=[download_progress])
    unload_btn.click(fn=lambda: (unload_model(), "âœ… æ˜¾å­˜å·²é‡Šæ”¾")[1], inputs=[], outputs=[status])
    generate_btn.click(
        fn=generate_video,
        inputs=[mode, input_image, input_video, input_audio, input_text],
        outputs=[output_video, output_status]
    )

if __name__ == "__main__":
    # å¯åŠ¨æ—¶æ£€æŸ¥å¹¶ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
    check_and_download_models()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

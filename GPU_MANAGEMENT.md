# GPU æ˜¾å­˜æ™ºèƒ½ç®¡ç†æ–‡æ¡£

## ğŸ¯ è®¾è®¡ç›®æ ‡

å®ç° **æ‡’åŠ è½½ + å³ç”¨å³å¸** çš„ GPU æ˜¾å­˜ç®¡ç†ç­–ç•¥ï¼Œåœ¨å¤š GPU ç¯å¢ƒä¸‹é«˜æ•ˆåˆ©ç”¨èµ„æºã€‚

## ğŸ“Š çŠ¶æ€è½¬æ¢å›¾

```
æœªåŠ è½½ â”€â”€é¦–æ¬¡è¯·æ±‚(20-30s)â”€â”€â†’ GPU â”€â”€ä»»åŠ¡å®Œæˆ(2s)â”€â”€â†’ CPU â”€â”€æ–°è¯·æ±‚(2-5s)â”€â”€â†’ GPU
  â†‘                                                      â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€è¶…æ—¶/æ‰‹åŠ¨é‡Šæ”¾(1s)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ å·¥ä½œæµç¨‹

### 1. æ‡’åŠ è½½ï¼ˆLazy Loadingï¼‰

æ¨¡å‹æŒ‰éœ€åŠ è½½ï¼Œé¿å…å¯åŠ¨æ—¶å ç”¨æ˜¾å­˜ï¼š

```python
# é¦–æ¬¡è¯·æ±‚
model = gpu_manager.get_model(load_func=load_infinitetalk_model)
# â†’ ä»ç£ç›˜åŠ è½½åˆ° GPU (20-30ç§’)

# ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆæ¨¡å‹åœ¨ CPUï¼‰
model = gpu_manager.get_model(load_func=load_infinitetalk_model)
# â†’ ä» CPU è½¬ç§»åˆ° GPU (2-5ç§’)

# ç¬¬ä¸‰æ¬¡è¯·æ±‚ï¼ˆæ¨¡å‹åœ¨ GPUï¼‰
model = gpu_manager.get_model(load_func=load_infinitetalk_model)
# â†’ ç›´æ¥è¿”å› (0ç§’)
```

### 2. å³ç”¨å³å¸ï¼ˆImmediate Offloadï¼‰

ä»»åŠ¡å®Œæˆåç«‹å³é‡Šæ”¾æ˜¾å­˜ï¼š

```python
def process_video(input_data):
    try:
        # æ­¥éª¤1: è·å–æ¨¡å‹
        model = gpu_manager.get_model(load_func=load_model)
        
        # æ­¥éª¤2: å¤„ç†
        result = model.process(input_data)
        
        # æ­¥éª¤3: ç«‹å³å¸è½½ï¼ˆå…³é”®ï¼ï¼‰
        gpu_manager.force_offload()
        
        return result
    except Exception as e:
        # å¼‚å¸¸æ—¶ä¹Ÿè¦å¸è½½
        gpu_manager.force_offload()
        raise e
```

### 3. è‡ªåŠ¨è¶…æ—¶

ç©ºé—²è¶…è¿‡è®¾å®šæ—¶é—´åè‡ªåŠ¨è½¬ç§»åˆ° CPUï¼š

```python
# é»˜è®¤ 60 ç§’è¶…æ—¶
gpu_manager = GPUResourceManager(idle_timeout=60)

# ç›‘æ§çº¿ç¨‹æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡
# å¦‚æœç©ºé—²æ—¶é—´ > idle_timeoutï¼Œè‡ªåŠ¨æ‰§è¡Œ _move_to_cpu()
```

## ğŸ› ï¸ API æ¥å£

### Python API

```python
from gpu_manager import get_gpu_manager

# è·å–å…¨å±€ç®¡ç†å™¨
gpu_manager = get_gpu_manager(idle_timeout=60)

# å¯åŠ¨ç›‘æ§
gpu_manager.start_monitor()

# è·å–æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰
model = gpu_manager.get_model(load_func=your_load_function)

# æ‰‹åŠ¨å¸è½½
gpu_manager.force_offload()

# å®Œå…¨é‡Šæ”¾
gpu_manager.force_release()

# è·å–çŠ¶æ€
status = gpu_manager.get_status()

# æ›´æ–°è¶…æ—¶
gpu_manager.update_timeout(300)
```

### REST API

```bash
# æŸ¥çœ‹ GPU çŠ¶æ€
curl http://localhost:7860/gpu/status

# æ‰‹åŠ¨å¸è½½
curl -X POST http://localhost:7860/gpu/offload

# å®Œå…¨é‡Šæ”¾
curl -X POST http://localhost:7860/gpu/release

# æ›´æ–°è¶…æ—¶
curl -X POST http://localhost:7860/gpu/timeout -F "timeout=300"
```

### MCP å·¥å…·

```python
# æŸ¥çœ‹çŠ¶æ€
await mcp_client.call_tool("get_gpu_status", {})

# æ‰‹åŠ¨å¸è½½
await mcp_client.call_tool("offload_gpu", {})

# å®Œå…¨é‡Šæ”¾
await mcp_client.call_tool("release_gpu", {})

# æ›´æ–°è¶…æ—¶
await mcp_client.call_tool("update_gpu_timeout", {"timeout_seconds": 300})
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### æ—¶é—´å¼€é”€

| æ“ä½œ | æ—¶é—´ | è¯´æ˜ |
|------|------|------|
| é¦–æ¬¡åŠ è½½ | 20-30ç§’ | ä»ç£ç›˜åŠ è½½æ¨¡å‹åˆ° GPU |
| CPUâ†’GPU | 2-5ç§’ | ä» CPU ç¼“å­˜æ¢å¤åˆ° GPU |
| GPUâ†’CPU | 2ç§’ | å¸è½½åˆ° CPUï¼Œé‡Šæ”¾æ˜¾å­˜ |
| å®Œå…¨é‡Šæ”¾ | 1ç§’ | æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ |

### æ˜¾å­˜å ç”¨

| çŠ¶æ€ | æ˜¾å­˜å ç”¨ | è¯´æ˜ |
|------|----------|------|
| æœªåŠ è½½ | 0 GB | æ— æ¨¡å‹ |
| CPU ç¼“å­˜ | < 0.5 GB | ä»…ä¿ç•™å°‘é‡ GPU ç¼“å­˜ |
| GPU è¿è¡Œ | 8-20 GB | æ ¹æ®æ¨¡å‹å¤§å° |

## ğŸ›ï¸ é…ç½®å‚æ•°

### ç¯å¢ƒå˜é‡

```bash
# GPU ç©ºé—²è¶…æ—¶ï¼ˆç§’ï¼‰
GPU_IDLE_TIMEOUT=60

# GPU è®¾å¤‡ ID
NVIDIA_VISIBLE_DEVICES=0
```

### ä»£ç é…ç½®

```python
# åˆå§‹åŒ–æ—¶è®¾ç½®
gpu_manager = GPUResourceManager(
    idle_timeout=60  # 60ç§’ç©ºé—²åè‡ªåŠ¨å¸è½½
)

# è¿è¡Œæ—¶æ›´æ–°
gpu_manager.update_timeout(300)  # æ”¹ä¸º 5 åˆ†é’Ÿ
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å•ä»»åŠ¡å¤„ç†

```python
def process_single_task(input_data):
    """æ ‡å‡†çš„å•ä»»åŠ¡å¤„ç†æµç¨‹"""
    try:
        model = gpu_manager.get_model(load_func=load_model)
        result = model.process(input_data)
        gpu_manager.force_offload()  # ç«‹å³å¸è½½
        return result
    except Exception as e:
        gpu_manager.force_offload()  # å¼‚å¸¸ä¹Ÿè¦å¸è½½
        raise e
```

### 2. æ‰¹é‡å¤„ç†

```python
def process_batch(input_list):
    """æ‰¹é‡å¤„ç†æ—¶å¤ç”¨æ¨¡å‹"""
    try:
        model = gpu_manager.get_model(load_func=load_model)
        
        results = []
        for input_data in input_list:
            result = model.process(input_data)
            results.append(result)
        
        # æ‰¹é‡å®Œæˆåå†å¸è½½
        gpu_manager.force_offload()
        return results
    except Exception as e:
        gpu_manager.force_offload()
        raise e
```

### 3. é•¿æ—¶é—´è¿è¡Œ

```python
def long_running_service():
    """é•¿æ—¶é—´è¿è¡Œçš„æœåŠ¡"""
    # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
    gpu_manager.update_timeout(600)  # 10 åˆ†é’Ÿ
    
    while True:
        task = get_next_task()
        if task:
            process_single_task(task)
        else:
            time.sleep(1)
```

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### æŸ¥çœ‹å®æ—¶çŠ¶æ€

```python
status = gpu_manager.get_status()
print(f"æ¨¡å‹ä½ç½®: {status['model_location']}")
print(f"ç©ºé—²æ—¶é—´: {status['idle_time']}ç§’")
print(f"æ˜¾å­˜å ç”¨: {status['gpu_memory_allocated_gb']:.2f}GB")
```

### æ—¥å¿—è¾“å‡º

```
INFO:gpu_manager:GPU ç›‘æ§å·²å¯åŠ¨ï¼Œç©ºé—²è¶…æ—¶: 60ç§’
INFO:gpu_manager:é¦–æ¬¡åŠ è½½æ¨¡å‹åˆ° GPU (20-30ç§’)
INFO:gpu_manager:åŠ è½½å®Œæˆï¼Œè€—æ—¶ 23.5ç§’
INFO:gpu_manager:ä»»åŠ¡å®Œæˆï¼Œå¸è½½æ¨¡å‹åˆ° CPU (2ç§’)
INFO:gpu_manager:å¸è½½å®Œæˆï¼Œè€—æ—¶ 1.8ç§’
INFO:gpu_manager:GPU æ˜¾å­˜: å·²åˆ†é… 0.45GB, å·²ä¿ç•™ 0.50GB
INFO:gpu_manager:ä» CPU æ¢å¤æ¨¡å‹åˆ° GPU (2-5ç§’)
INFO:gpu_manager:æ¢å¤å®Œæˆï¼Œè€—æ—¶ 3.2ç§’
INFO:gpu_manager:æ¨¡å‹ç©ºé—² 65ç§’ï¼Œè‡ªåŠ¨å¸è½½åˆ° CPU
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: æ˜¾å­˜æœªé‡Šæ”¾

**ç—‡çŠ¶ï¼š** è°ƒç”¨ `force_offload()` åæ˜¾å­˜ä»ç„¶å¾ˆé«˜

**åŸå› ï¼š** å¯èƒ½æœ‰å…¶ä»–å¼•ç”¨æœªé‡Šæ”¾

**è§£å†³ï¼š**
```python
# ç¡®ä¿æ²¡æœ‰å…¶ä»–å˜é‡å¼•ç”¨æ¨¡å‹
model = None
gpu_manager.force_release()  # ä½¿ç”¨å®Œå…¨é‡Šæ”¾

# æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶
import gc
gc.collect()
torch.cuda.empty_cache()
```

### é—®é¢˜2: æ¢å¤é€Ÿåº¦æ…¢

**ç—‡çŠ¶ï¼š** CPUâ†’GPU æ¢å¤è¶…è¿‡ 10 ç§’

**åŸå› ï¼š** CPU å†…å­˜ä¸è¶³æˆ–æ¨¡å‹å¤ªå¤§

**è§£å†³ï¼š**
```python
# æ–¹æ¡ˆ1: å¢åŠ ç³»ç»Ÿå†…å­˜
# æ–¹æ¡ˆ2: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
# æ–¹æ¡ˆ3: å®Œå…¨é‡Šæ”¾åé‡æ–°åŠ è½½
gpu_manager.force_release()
```

### é—®é¢˜3: è‡ªåŠ¨å¸è½½ä¸å·¥ä½œ

**ç—‡çŠ¶ï¼š** è¶…æ—¶åæ¨¡å‹ä»åœ¨ GPU ä¸Š

**åŸå› ï¼š** ç›‘æ§çº¿ç¨‹æœªå¯åŠ¨

**è§£å†³ï¼š**
```python
# ç¡®ä¿å¯åŠ¨ç›‘æ§
gpu_manager.start_monitor()

# æ£€æŸ¥ç›‘æ§çŠ¶æ€
print(f"ç›‘æ§è¿è¡Œä¸­: {gpu_manager.running}")
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### ä¼ ç»Ÿæ–¹å¼ vs æ™ºèƒ½ç®¡ç†

| æŒ‡æ ‡ | ä¼ ç»Ÿæ–¹å¼ | æ™ºèƒ½ç®¡ç† | æå‡ |
|------|----------|----------|------|
| å¯åŠ¨æ—¶é—´ | 30ç§’ | 0ç§’ | âœ… å³æ—¶å¯åŠ¨ |
| ç©ºé—²æ˜¾å­˜ | 20GB | <1GB | âœ… èŠ‚çœ 95% |
| ç¬¬äºŒæ¬¡è¯·æ±‚ | 0ç§’ | 3ç§’ | âš ï¸ ç•¥æ…¢ |
| GPU åˆ©ç”¨ç‡ | ä½ | é«˜ | âœ… å¤šå®¹å™¨å…±äº« |

## ğŸ“ è¿›é˜¶æŠ€å·§

### 1. å¤šæ¨¡å‹ç®¡ç†

```python
# ä¸ºä¸åŒæ¨¡å‹åˆ›å»ºä¸åŒçš„ç®¡ç†å™¨
infinitetalk_manager = GPUResourceManager(idle_timeout=60)
multitalk_manager = GPUResourceManager(idle_timeout=120)

# åˆ†åˆ«ç®¡ç†
model1 = infinitetalk_manager.get_model(load_infinitetalk)
model2 = multitalk_manager.get_model(load_multitalk)
```

### 2. ä¼˜å…ˆçº§é˜Ÿåˆ—

```python
# é«˜ä¼˜å…ˆçº§ä»»åŠ¡ç«‹å³åŠ è½½
if task.priority == "high":
    gpu_manager.force_offload()  # å…ˆå¸è½½å…¶ä»–æ¨¡å‹
    model = gpu_manager.get_model(load_func)
```

### 3. é¢„çƒ­ç­–ç•¥

```python
# é¢„æµ‹å³å°†ä½¿ç”¨ï¼Œæå‰åŠ è½½åˆ° GPU
def preheat():
    model = gpu_manager.get_model(load_func)
    # ä¸ç«‹å³å¸è½½ï¼Œç­‰å¾…å®é™…è¯·æ±‚
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [MCP ä½¿ç”¨æŒ‡å—](MCP_GUIDE.md)
- [API æ–‡æ¡£](http://localhost:7860/docs)
- [é¡¹ç›® README](README.md)
